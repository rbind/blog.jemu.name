---
title: Some Light Web-Scraping of the New ATP Site
author: jemus42
date: "2021-11-02"
slug: some-light-web-scraping-of-the-new-atp-site
series:
  - R
tags:
  - "web scraping"
  - "podcasts"
featured_image: "plots/links-histo-1.png"
description: 'Trying my hand at a little more web-scraping. Just gathering some data about ATP podcast episodes, not because itâ€™s useful, but because itâ€™s possible.'
packages:
  - polite
  - rvest
toc: yes
math: no
draft: true
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, cache=FALSE}
source(here::here("R/post-setup.R"))
knitr::opts_chunk$set(cache = FALSE)

if (file.exists("atp_episodes.rds")) {
  atp_episodes <- readRDS("atp_episodes.rds")
}
```

## Introduction

{{< addendum title="Note" >}}
I started writing this post in June 2020, and it has been in "I should get back to that"-limbo for over ~~6~~ ~~8~~ ~~12~~ ~~13 months~~ a long time becauseâ€¦ well, the ~~year's~~ time's been busy.

The code for the scrapey bits has since been put into its own [little package](https://github.com/jemus42/poddr) â€” so if you want to just get some data, you can install that or get the up-to-date data [from my other project's site](https://podcasts.jemu.name/data/).
{{< /addendum >}}

```{r pkgs, chunk_fold="Packages & setup"}
# For web-scraping
library(polite)
library(rvest)

# For convenience
library(dplyr)
library(tidyr)
library(stringr)
library(lubridate)

# Tables
library(kableExtra)

# Plotting
library(ggplot2)
library(tadaathemes) # remotes::install_github("tadaadata/tadaathemes")

plot_caption <- glue::glue("@jemus42 // {Sys.Date()}")
```

There are two main "tricks" to successful web-scraping (in my admittedly limited experience):

1. Get as close to the information you care about by using / identifying appropriate CSS selectors.
2. Whittle down text content as needed by trying to not have regular expressions wear you down.

For step 1, I like using [SelectorGadget], but you may be more comfortable using your favorite browser's developer tools, or looking at the site's HTML source, or being *really* good at guessing.  
For point 2, you'll probably want to spend some time on (and bookmark) [regexr] or [regex101], and maybe you'll like the {{< pkg "rematch2" >}} package. I haven't tried the latter yet, but I tend to think "I should look at that some time" every once in a while before I go back to my regular regexing.

Then there's **step 0** of web scraping:  
*Don't be a jerk to the site you're scraping*.  
In this case I don't think there's too much of a chance of me hammering the site in question too much, as I learned I'm going to need a total of *10* pages.
Soâ€¦ I don't think I could do too much damage even if I tried. However, I will still use the {{< pkg "polite" >}} package because I wanted to try it out anyway, and from what I gather it's explicit purpose is to, well, be *polite* with your scraping while still being compatible with your standard {{< pkg "rvest" >}} workflow.

Oh, and I totally forgot **step -1** of web scraping:  
*Make sure you actually have permission*. 
There's plenty of sites out there that don't allow third parties to scrape their content without expressed permission. 
Read the TOS if applicable, and make sure you're not playing the *"but it's a free website why am I not entitled to scraping all its content and use it for my own purposes"* card.  
In this particular case, I'm fairly certain that tabulating podcast episodes and shownote URLs is "safe" [^sorrymarco].    
Things are different for popular scraping targets like [IMDb](https://www.imdb.com/conditions).


> **Robots and Screen Scraping**: You may not use data mining, robots, screen scraping, or similar data gathering and extraction tools on this site, except with our express written consent as noted below.

So, even if you were extra polite in your IMDb scraping, you're still being a jerk by breaking their TOS, so you're going to have to get that data [through their official channel](https://developer.imdb.com/) for your projects. 

[SelectorGadget]: https://selectorgadget.com/
[regexr]: https://regexr.com/
[regex101]: https://regex101.com/
[^sorrymarco]: Please don't hate me Marco ðŸ¥º

Anyway, let's get started with a basic scraping setup: I'm going to get the first page of [atp.fm](https://atp.fm) and collect all the links, separated into text and URL:

```{r data-getting}
session <- bow("https://atp.fm/", force = TRUE)
links <- scrape(session) |>
  html_nodes("li a") 

atp_links <- tibble::tibble(
  text = html_text(links),
  url = html_attr(links, "href")
)

head(atp_links)
```

Welp, that's pretty straight forward. It's made fairly easy by the fact that all the shownote links are list items (`<li>`), but of course it would be nicer if we could match links to the episode they belong to.  
We can do that by iterating over all the `<article>` elements in the page, which enclose each episode post. 
As an example, let's get the episode number/titles of the most recent episodes:

```{r}
episode_titles <- scrape(session) |>
  html_nodes("article") |>
  html_nodes("h2 a") |>
  html_text()

tibble::tibble(
  number = str_extract(episode_titles, "^\\d+"),
  episode = str_remove(episode_titles, "^\\d+:\\s")
)
```

This is also the first case of regex making things a little neater in the result but harder to grasp along the way. If you've been spared the regex way of life until now, what we did here breaks down to this:

1. Take the episode title, e.g. `"437: The Right Side of the Mouse Pad"` from the HTML via `html_text()`
2. *Extract* the number by taking the non-zero amount of digits (`\\d+`) from the beginning of the string (`^`)
3. *Remove* that same number, including a `:` and an extra whitespace (`\\s`) to be left with the episode title.

Thankfully {{< pkg "stringr" >}} makes this nice and readable.  
And now we canâ€¦ go all out. Figure out all the elements of the site we're interested in regarding episode metadata and links, and put all the scrapey bits into a function for convenience. 
I won't explain each element, but feel free to comment if you don't understand a specific part. In any case, with the minimal amount of setup required you can easily trial-and-error your way through this part if you're into that[^butnote].

[^butnote]: But please note that you don't have to re-read the website in question to scrape it. You can save the website content to a variable and then use that as the base for all your `html_nodes` and regex experiments.

```{r assemble-episodes, chunk_fold="atp_parse_page()"}
# Here, page is an object as returned by polite::scrape()
atp_parse_page <- function(page) {
  rvest::html_nodes(page, "article") |>
    # Iterate over all the article elements (episodes) on the page
    # Makes it easier to collect elements corresponding to each episode
    purrr::map_dfr(~ {
      # Extract the metadata block, we'll take that apart in steps
      meta <- rvest::html_node(.x, ".metadata") |>
        rvest::html_text() |>
        stringr::str_trim()

      date <- meta |>
        stringr::str_extract("^.*(?=\\\n)") |>
        lubridate::mdy()

      duration <- meta |>
        stringr::str_extract("\\d{2}:\\d{2}:\\d{2}") |>
        hms::as_hms()

      number <- .x |>
        rvest::html_nodes("h2 a") |>
        rvest::html_text() |>
        stringr::str_extract("^\\d+")

      title <- .x |>
        rvest::html_nodes("h2 a") |>
        rvest::html_text() |>
        stringr::str_remove("^\\d+:\\s")

      # Get the sponsor links
      links_sponsor <- .x |>
        # Shownotes links are in the second <ul> element
        rvest::html_nodes("ul~ ul li") |>
        rvest::html_nodes("a")

      link_text_sponsor <- links_sponsor |>
        rvest::html_text()

      link_href_sponsor <- links_sponsor |>
        rvest::html_attr("href")

      links_sponsor <- tibble(
        link_text = link_text_sponsor,
        link_url = link_href_sponsor,
        link_type = "Sponsor"
      )

      # Get the regular shownotes links
      links_regular <- .x |>
        # Get the first <ul> element, then the listed links
        # This avoids links in paragraphs and shownotes
        rvest::html_node("ul") |>
        rvest::html_nodes("li a")

      link_text <- links_regular |>
        rvest::html_text()

      link_href <- links_regular |>
        rvest::html_attr("href")

      links_regular <- tibble(
        link_text = link_text,
        link_url = link_href,
        link_type = "Shownotes"
      )
      # Piece it all together all tibbly
      # Links will be a list-column
      tibble(
        number = number,
        title = title,
        duration = duration,
        date = date,
        year = lubridate::year(date),
        month = lubridate::month(date, abbr = FALSE, label = TRUE),
        weekday = lubridate::wday(date, abbr = FALSE, label = TRUE),
        links = list(dplyr::bind_rows(links_regular, links_sponsor)),
        n_links = nrow(links_regular) + nrow(links_sponsor)
      )
    })
}
```

Now we have a compact way to get all the interesting bits in a neat tibble, including a list-column with shownote links separated into sponsor- and topical URLs:

```{r sponsor-link-looksy}
scraped_page <- scrape(session) |>
  atp_parse_page()

glimpse(scraped_page)

scraped_page$links[[1]]
```

Now what's left is to get _all_ the episodes, because why not.

## Getting _All_ the Episodes

Or: This is my first `while`-loop in R sinceâ€¦ I think 2015?  
I don't regret *everything* per se, but I think this is actually reasonable.

```{r scrape-atp, eval=!file.exists("atp_episodes.rds"), chunk_fold="atp_get_episodes()"}
atp_get_episodes <- function(page_limit = NULL) {

  # If there's no page limit, we set it to infinity
  # because it's easier if it's a number and I'm bad at while loops
  if (is.null(page_limit)) page_limit <- Inf

  # Get the first page and scrape it
  session <- polite::bow(url = "https://atp.fm")

  atp_pages <- list("1" = polite::scrape(session))
  next_page_num <- 2

  # Early return for first page only
  if (page_limit == 1) {
    atp_parse_page(atp_pages[[1]])
  }

  # Find out how many pages there will be in total
  # purely for progress bar cosmetics.
  latest_ep_num <- atp_pages[[1]] |>
    rvest::html_nodes("h2 a") |>
    rvest::html_text() |>
    stringr::str_extract("^\\d+") |>
    as.numeric() |>
    max()

  # First page has 5 episodes, 50 episodes per page afterwards
  total_pages <- ceiling((latest_ep_num - 5) / 50) + 1

  # Everything is better with progress bars.
  pb <- progress::progress_bar$new(
    format = "Getting pages [:bar] :current/:total (:percent) ETA: :eta",
    total = total_pages
  )
  pb$tick()

  # Iteratively get the next page until the limit is reached
  # (or of there's no next page to retrieve)
  while (next_page_num <= page_limit) {
    pb$tick()

    atp_pages[[next_page_num]] <- polite::scrape(
      session,
      query = list(page = next_page_num)
    )

    # Find the next page number
    next_page_num <- atp_pages[[next_page_num]] |>
      rvest::html_nodes("#pagination a+ a") |>
      rvest::html_attr("href") |>
      stringr::str_extract("\\d+$") |>
      as.numeric()

    # Break the loop if there's no next page
    if (length(next_page_num) == 0) break
  }

  # Now parse all the pages and return
  pb <- progress::progress_bar$new(
    format = "Parsing pages [:bar] :current/:total (:percent) ETA: :eta",
    total = length(atp_pages)
  )

  purrr::map_dfr(atp_pages, ~ {
    pb$tick()
    atp_parse_page(.x)
  })
}

# Parse all the pages
atp_episodes <- atp_get_episodes()
```

```{r cache-atp-eps, include=FALSE, eval=!file.exists("atp_episodes.rds")}
saveRDS(atp_episodes, "atp_episodes.rds")
```

```{r read-atp-eps, include=FALSE, eval=file.exists("atp_episodes.rds")}
atp_episodes <- readRDS("atp_episodes.rds")
```

## Looking at Links

One of the neat things we can do with this ATP data compared to [other podcast data I've scraped in the past](https://podcasts.jemu.name/) is the inclusion of nicely formatted shownote links. 
So we might as well take a closer look at what we've got there.

```{r links-histo}
ggplot(atp_episodes, aes(x = n_links)) +
  geom_bar(alpha = .75) +
  scale_x_binned() +
  labs(
    title = "ATP.fm: Number of Links per Episode",
    x = "# of Links in Episode Shownotes",
    y = "Episodes",
    caption = plot_caption
  ) + 
  theme_tadaark()
```

Huh, what's with the right outlier?

```{r links-top}
atp_episodes |>
  slice_max(n_links, n = 5) |>
  select(date, number, title, n_links) |>
  kable(caption = "Episodes with most links")
```

Okay, so [episode 119][e119] *wins*. But it seems unfair to not take episode length into account, so let's try that.

[e119]: https://atp.fm/119

```{r links-most-common-target}
atp_episodes |>
  unnest(links) |>
  count(link_url, link_type, sort = TRUE) |>
  group_by(link_type) |>
  slice_max(n, n = 3)
```


```{r links-duration-scatter}
ggplot(atp_episodes, aes(x = duration, y = n_links)) +
  geom_point() +
  labs(
    title = "ATP.fm: Number of Links per Episode",
    x = "Episode Duration (H:M:S)",
    y = "Number of Links in Show Notes",
    caption = plot_caption
  ) +
  theme_tadaark()
```

Well okay, there's an unsurprising trend there. Maybe we should take a look at `links / minute`, as a metric of how much linkage there is being done.

```{r links-per-minute}
atp_episodes <- atp_episodes |>
  mutate(lpm = n_links / (as.numeric(duration) / 60))

atp_episodes |>
  slice_max(lpm, n = 5) |> 
  select(date, title, duration, n_links, lpm) |>
  kable(caption = "Episodes with most links per minute")
```


```{r links-duration-monthly}
atp_episodes |>
  ggplot(aes(x = month, y = n_links)) +
  geom_boxplot(alpha = .25, fill = "#374453") +
  geom_point(stat = "summary", fun = mean, fill = "lightblue", shape = 21, size = 3) + 
  theme_tadaark()
```

### URL Protocol

I don't know if you've heard the news yet, but HTTPS is kind of a big deal.  
As a matter of fact, it has become more and more of a big deal over the past few years, and one way to illustrate that is the ratio of HTTP/HTTPS links posted on the web.
So guess what we're doing with the URL data next.


```{r links-proto}
atp_episodes_links <- atp_episodes |>
  unnest(links) |>
  mutate(
    HTTPS = ifelse(str_detect(link_url, "^https"), "https", "http"),
    domain = urltools::domain(link_url) |>
      str_remove_all("www\\.")
  ) 

atp_episodes_links |>
  ggplot(aes(x = HTTPS)) +
  geom_bar(alpha = .75, color = "white") + 
  theme_tadaark()

atp_episodes_links |>
  count(year = lubridate::year(date), HTTPS) |>
  group_by(year) |>
  mutate(prop = n / sum(n)) |>
  ggplot(aes(x = year, y = prop, fill = HTTPS)) +
  geom_col(alpha = .75, color = "white") + 
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  hrbrthemes::scale_y_percent() +
  theme_tadaark()

atp_episodes_links |>
  count(year = lubridate::year(date), HTTPS, link_type) |>
  group_by(link_type, year) |>
  mutate(prop = n / sum(n)) |>
  ggplot(aes(x = year, y = prop, fill = HTTPS)) +
  facet_wrap(vars(link_type)) +
  geom_col(alpha = .75, color = "white") + 
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  hrbrthemes::scale_y_percent() +
  theme_tadaark()
```


```{r links-proto-2020}
atp_episodes_links |>
  filter(year(date) == 2020, HTTPS == "http") |>
  count(domain, link_type, sort = TRUE) |>
  head(10) |>
  kable(
    col.names = c("Domain", "Link Type", "# of HTTP links")
  )
```

### Domains

```{r links-domains}
atp_episodes_links |>
  filter(link_type == "Shownotes") |>
  count(domain, sort = TRUE)
```

```{r links-domains-year}
atp_episodes_links |>
  filter(link_type == "Shownotes") |>
  count(year = year(date), domain, sort = TRUE) |>
  group_by(year) |>
  slice_max(n, n = 5) |>
  kable() |>
  kable_styling() |>
  collapse_rows(1)
```


## Conclusion

```{r sessioninfo, chunk_fold="Session Info"}
sess <- sessioninfo::session_info()

sess$platform |>
  unclass() |>
  tibble::as_tibble() |>
  t() |>
  knitr::kable() |>
  kableExtra::kable_styling()

sess$packages |>
  tibble::as_tibble() |>
  dplyr::select(package, version = ondiskversion, source) |>
  knitr::kable() |>
  kableExtra::kable_styling()
```
